# -*- coding: utf-8 -*-
"""instagram scraper.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hxrJy0u12iZBsN6e1wRdxxU1IBMbhfka
"""

import pandas as pd
import numpy
import seaborn
import json
import datetime
from collections import defaultdict
import urllib.request
import os
from urllib.request import Request, urlopen
import shutil

def parse_instagram_json(json_file_path, user_handle, path = 'drive/MyDrive/scraped_data_instagram/'):

  with open(json_file_path) as json_file:
    data = json.load(json_file)
 
  video = data['graphql']['user']['edge_felix_video_timeline']['edges']

  video_dict = defaultdict(list)

  for i in range(len(video)):
    url = video[i]['node']['video_url']
    video_dict['url'].append(url)
    timestamp = datetime.datetime.fromtimestamp(video[i]['node']['taken_at_timestamp'])
    video_dict['timestamp'].append(timestamp)  

    filename = user_handle + '_' + timestamp.strftime('%Y%m%d') + '_vid.mp4'
    video_dict['filename'].append(filename)

    try:
      urllib.request.urlretrieve(url, os.path.join(path, filename))
    except:
      print('url not valid')

    if len(video[i]['node']['edge_media_to_caption']['edges']) > 0:
      video_dict['caption'].append(video[i]['node']['edge_media_to_caption']['edges'][0]['node']['text'])
    else:
      video_dict['caption'].append(user_handle)

    video_dict['type'].append('video')
    video_dict['account'].append(user_handle)

  graphics = data['graphql']['user']['edge_owner_to_timeline_media']['edges']

  graphics_dict = defaultdict(list)

  for i in range(len(graphics)):
    timestamp = datetime.datetime.fromtimestamp(graphics[i]['node']['taken_at_timestamp'])
    if len(graphics[i]['node']['edge_media_to_caption']['edges']) > 0:
      caption = graphics[i]['node']['edge_media_to_caption']['edges'][0]['node']['text']
    else:
      caption = ''
    
    if 'edge_sidecar_to_children' in graphics[i]['node'].keys():
      images = graphics[i]['node']['edge_sidecar_to_children']['edges']

      for j in range(len(images)):
        url = images[j]['node']['display_url']
        graphics_dict['url'].append(url)
        filename = user_handle + '_' + timestamp.strftime('%Y%m%d') + '_' + str(j) + '_' +'_img.jpeg'
        graphics_dict['filename'].append(filename)
        
        req = Request(
            url=url,
            headers={'User-Agent': 'Mozilla/5.0'}
          )
        try:
          image = urlopen(req)
          shutil.copyfileobj(image, open(os.path.join(path, filename), 'wb'))
        except:
          print('url not valid')

        
        graphics_dict['timestamp'].append(timestamp)
        graphics_dict['caption'].append(caption)
        graphics_dict['type'].append('image')
        graphics_dict['account'].append(user_handle)

  pd.concat([pd.DataFrame.from_dict(graphics_dict),pd.DataFrame.from_dict(video_dict)]).to_csv(os.path.join(path, user_handle + '.csv'))

json_file_path_ns = '/content/drive/MyDrive/Scraper/cn_daily.json'
user_handle_ns = 'citizensdailycn'

parse_instagram_json(json_file_path_ns,  user_handle_ns, path = 'drive/MyDrive/scraped_data_instagram/')

json_file_path_ns = '/content/drive/MyDrive/Scraper/northern square.json'
user_handle_ns = 'northernsquare'

parse_instagram_json(json_file_path_ns,  user_handle_ns, path = 'drive/MyDrive/scraped_data_instagram/')

